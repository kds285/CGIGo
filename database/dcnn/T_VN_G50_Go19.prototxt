name: "Go19_G50_VN"

layer {
  name: "data_input_layer"
  type: "MemoryData"
  top: "data"
  top: "dummy_data"
  memory_data_param {
    batch_size: 16
  	channels: 50
    height: 19
    width: 19
  }
}

layer {
  name: "label_input_layer"
  type: "MemoryData"
  top: "label"
  top: "dummy_label"
  memory_data_param {
    batch_size: 16
#########################
  	channels: 1
#########################
    height: 1
    width: 1
  }
}

layer {
	name: "silence_useless_label"
	type: "Silence"
	bottom: "dummy_data"
	bottom: "dummy_label"
}

#################################################################
#this part should be the same in learning and prediction network#
#################################################################

layer {
  name: "conv1_5x5_128"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 128
    kernel_size: 5
    pad: 2
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}

#=======================================

layer {
  name: "conv2_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv1"
  top: "conv2"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
   std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}

#=======================================

layer {
  name: "conv3_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv2"
  top: "conv3"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}

#=======================================

layer {
  name: "conv4_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}

#=======================================

layer {
  name: "conv5_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}

#=======================================

layer {
  name: "conv6_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv5"
  top: "conv6"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
   std: 0.01
    }
    bias_filler {
      type: "constant"
 value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "conv6"
  top: "conv6"
}

#=======================================

layer {
  name: "conv7_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv6"
  top: "conv7"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "conv7"
}

#=======================================

layer {
  name: "conv8_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv7"
  top: "conv8"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "conv8"
}

#=======================================

layer {
  name: "conv9_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv8"
  top: "conv9"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
}

#=======================================

layer {
  name: "conv10_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv9"
  top: "conv10"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu10"
  type: "ReLU"
  bottom: "conv10"
  top: "conv10"
}

#=======================================

layer {
  name: "conv11_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv10"
  top: "conv11"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}

#=======================================

layer {
  name: "conv12_3x3_192"
  type: "Convolution"
  param{
    lr_mult: 1
  }
  param{
    lr_mult: 2
  }
  bottom: "conv11"
  top: "conv12"
  convolution_param {
    num_output: 192
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}
layer {
  name: "relu12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}


#=======================================

layer {
  name: "conv13_1x1_1"
  type: "Convolution"
  bottom: "conv12"
  top: "conv13"
  param{
    lr_mult: 1
  }
  param{
  	lr_mult: 2
  }

  convolution_param {
    num_output: 1
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "gaussian"
	  std: 0.01
    }
    bias_filler {
      type: "constant"
	  value: 0.1
    }
  }
}

layer {
  name: "relu13"
  type: "ReLU"
  bottom: "conv13"
  top: "conv13"
}

#=======================================

layer {
  name: "fc14"
  type: "InnerProduct"
  bottom: "conv13"
  top: "fc14"
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
    std: 0.027
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu14"
  type: "ReLU"
  bottom: "fc14"
  top: "fc14"
}

#=======================================

layer {
  name: "fc15"
  type: "InnerProduct"
  bottom: "fc14"
  top: "fc15"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "gaussian"
    std: 0.027
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "tanh15"
  type: "TanH"
  bottom: "fc15"
  top: "tanh"
}

layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "tanh"
  bottom: "label"
  top: "loss"
}
